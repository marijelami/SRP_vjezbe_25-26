# ğŸ“Š ETL Pipeline â€“ Pregled

Ovaj dokument opisuje ETL (Extract, Transform, Load) proces koriÅ¡tenjem Apache Sparka i PySparka. Podaci se prikupljaju iz dvije izvore: relacijske baze podataka i CSV datoteke, transformiraju i uÄitavaju u skladiÅ¡te podataka.

---

## ğŸ§© Faze ETL procesa

### 1. ğŸ“¥ Ekstrakcija (Extract)

**Cilj:** Prikupljanje sirovih podataka iz:

- âœ… **Relacijske baze podataka** (npr. PostgreSQL, MySQL)
- âœ… **CSV datoteke** (lokalna pohrana ili cloud)

**Alati/metode:**
- `spark.read.format("jdbc")` â€“ za Äitanje iz baze
- `spark.read.csv()` â€“ za uÄitavanje CSV datoteke

---

### 2. ğŸ”§ Transformacija (Transform)

**Cilj:** Priprema podataka za uÄitavanje:

- ÄŒiÅ¡Ä‡enje (null vrijednosti, duplikati)
- Promjena tipova podataka
- Filtriranje i odabir relevantnih podataka
- Spajanje podataka iz baze i CSV-a (ako je potrebno)
- Poslovna logika (npr. agregacije, izraÄuni)
- Preimenovanje ili reorganizacija kolona

**Alati/metode:**
- `.withColumn()`, `.select()`, `.filter()`, `.join()`, `.groupBy()` i druge PySpark transformacije

---

### 3. ğŸ“¤ UÄitavanje (Load)

**Cilj:** UÄitavanje obraÄ‘enih podataka u skladiÅ¡te podataka (data warehouse)

**MoguÄ‡nosti:**
- Izvoz u Parquet/CSV na objektno spremiÅ¡te (npr. S3)
- Direktno pisanje u bazu putem JDBC konekcije
- KoriÅ¡tenje specifiÄnih konektora (npr. Redshift, Snowflake, BigQuery)

**Alati/metode:**
- `write.format("jdbc")`
- `write.mode("overwrite").save()`
- SkladiÅ¡te-specifiÄni konektori

---

## ğŸ§± Arhitektura Pipelinea

| Korak | Zadatak | Tehnologija / metoda |
|-------|---------|-----------------------|
| 1 | ÄŒitanje iz relacijske baze | `spark.read.format("jdbc")` |
| 2 | ÄŒitanje CSV datoteke | `spark.read.csv()` |
| 3 | ÄŒiÅ¡Ä‡enje i priprema podataka | PySpark transformacije |
| 4 | Spajanje podataka | `.join()` |
| 5 | Poslovna logika i obrada | `.withColumn()`, `.groupBy()`, itd. |
| 6 | UÄitavanje u skladiÅ¡te podataka | `write.format("jdbc")` ili konektor |

---

## âœ… Napomene

- Svi koraci se mogu pokretati lokalno ili na Sparku u klasteru
- CSV datoteka moÅ¾e biti lokalna ili pohranjena na oblaku
- VaÅ¾no je osigurati da je struktura podataka kompatibilna sa skladiÅ¡tem podataka

---

